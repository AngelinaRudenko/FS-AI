\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{enumitem}
% \usepackage{pdfpages}

\setlength{\parskip}{0.9em}
\setlength{\parindent}{0pt}

\title{Artificial Intelligence -- Semester Project \\[0.5em]
\large Data Analysis and Application of a Selected Machine Learning Method}
\author{Anhelina Rudzenka rudzeanh@fel.cvut.cz}
\date{\today}

\begin{document}
\maketitle

% --------------------------------------------------------------------

\section{Introduction (0.5--1 page)}
\begin{itemize}[leftmargin=*]
    \item Brief introduction to the selected method and problem.
    \item Motivation for dataset choice and its relevance.
    \item Structure of the report.
\end{itemize}

% --------------------------------------------------------------------

\section{Dataset Description}

\textbf{Dataset Name:} Flight Price Prediction \\
\textbf{Source:} Kaggle \\
\textbf{URL:} \href{https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction/data}{Kaggle Dataset Link}

This dataset contains information about flight booking options from the website "Ease My Trip" for travel between India's top 6 metro cities.
The data shows different flight options with their prices, airlines, departure times, and other details.
Data was collected for 50 days, from February 11th to March 31st, 2022.

The original dataset includes 3 CSV files:

\begin{itemize}[leftmargin=*]
    \item \textbf{business.csv} -- 93487 business class flight records
    \item \textbf{economy.csv} -- 206774 economy class flight records
    \item \textbf{Clean\_Dataset.csv} -- 300153 flight records (combined and cleaned dataset)
\end{itemize}

For this project, I used the \textbf{Clean\_Dataset.csv} file because it combines both business and economy class flight records into a single dataset and has already been cleaned, with 108 invalid rows removed.

The dataset consists of approximately 69\% economy class flights and 31\% business class flights, indicating a moderate imbalance toward economy class bookings that reflects real-world travel patterns.

The dataset has 11 columns that describe each flight:

\begin{itemize}[leftmargin=*]
    
    \item \textbf{airline} -- Airline company name 
    (6 airlines: SpiceJet, AirAsia, Vistara, GO\_FIRST, Indigo, Air\_India).

    \item \textbf{flight} -- Flight code or flight number  
    (1561 unique flight codes).

    \item \textbf{source\_city} -- Departure city  
    (6 cities: Delhi, Mumbai, Bangalore, Kolkata, Hyderabad, Chennai).

    \item \textbf{departure\_time} -- Time of day when the flight departs  
    (6 time periods: Early\_Morning, Morning, Afternoon, Evening, Night, Late\_Night).

    \item \textbf{stops} -- Number of stops during the flight  
    (3 categories: zero, one, two\_or\_more).

    \item \textbf{arrival\_time} -- Time of day when the flight arrives  
    (Same six time periods as the departure time).

    \item \textbf{destination\_city} -- City where the flight arrives  
    (Same six cities as the source city).

    \item \textbf{class} -- Type of ticket  
    (2 classes: economy, business).

    \item \textbf{duration} -- Total flight duration in hours  
    (Range: 0.83 to 49.83 hours, with 476 distinct values).

    \item \textbf{days\_left} -- Number of days remaining until departure  
    (Range: 1 to 49 days).

    \item \textbf{price} -- Flight ticket price in Indian Rupees.

\end{itemize}

There are three numerical features (\texttt{duration}, \texttt{days\_left}, \texttt{price}), while all other features are categorical text data.

During the exploratory analysis, I found the following key points:

\begin{itemize}
    \item The mean ticket price is 20889.66, while the median price is 7425.00. This shows that the price distribution is highly skewed, with some very expensive tickets that increase the average price.
    \item Most passengers travel in the economy class.
    \item Most flights have one stop.
    \item There are very few late-night flights.
\end{itemize}

During the identification of dataset issues, I found the following:

\begin{itemize}
    \item The dataset is fully complete, with no missing values in any of the columns.
    \item The data do not contain significant outliers. Less than 1\% of the observations are outliers.
    \item None of the numerical features (\texttt{duration}, \texttt{days\_left}, \texttt{price}) are highly correlated with each other.
    \item There is a strong class imbalance in the \texttt{flight} and \texttt{departure\_time} columns. The imbalance ratios are approximately 54.48:1 for \texttt{departure\_time} and 3235:1 for \texttt{flight}.
    \item For the \texttt{departure\_time} column, morning flights account for 24\% of the data, while late-night flights represent only 0.4\%. This suggests that people tend to avoid late flights.
    \item For the \texttt{flight} column, this likely means that some flights operate much more often than others.
\end{itemize}

The plots used for the exploratory analysis and dataset issues identification are provided in the appendix.
I also generated a data profile using \texttt{ydata-profiling}, which is also included in the appendix.

% --------------------------------------------------------------------

\section{Problem Definition}

The problem can be formulated as follows: given information about a flight, we want to predict the ticket price in Indian Rupees (INR).

This is a supervised learning problem, because the dataset contains labeled data (flights with known prices). More specifically, this is a regression task, because we predict the price, which is a continuous numerical value.

For this project, I chose to use Random Forest Regression as the main prediction algorithm. 
Random Forest Regression builds many decision trees and combines their predictions to make a strong model.
This choice is justified by several reasons.

First, works well with both numerical and categorical features, which matches the dataset.

Second, Random Forest can capture non-linear relationships. The relationship between features and price is not always linear.

Fourth, Random Forest handles feature interactions automatically. The price may depend on combinations of features, and the model discovers these interactions during training without manual specification.

Finally, the model is less prone to overfitting compared to a single decision tree.

% --------------------------------------------------------------------

\section{Data Preprocessing}

During the exploratory data analysis in the previous chapter, I checked the dataset for common problems. Here, I describe the issues found and how they were addressed.

\textbf{Outliers}\\
\textit{Issue:} Some flights had unusually long durations or very high prices.\\
\textit{Action:} I kept all outliers in the dataset. This is because Random Forest models handle outliers well, and these extreme values represent real scenarios, such as multi-stop journeys or business class tickets peak times. Removing them could discard useful information about expensive flight options.

\textbf{Irrelevant Features}\\
\textit{Issue:} The \texttt{Unnamed: 0} column was only an index. The \texttt{flight} column contained flight codes, which had too many unique values and mostly reflected historical data, not useful for learning general price patterns.\\
\textit{Action:} Both columns were removed. This reduced noise and made the data simpler for the model.

I applied the following preprocessing steps:

\begin{enumerate}
    \item \textbf{Feature Selection:} The \texttt{Unnamed: 0} and \texttt{flight} columns were removed as they were irrelevant.
    
    \item \textbf{Separating Features and Target Variable:} The dataset was split into features ($X$) and the target variable ($y$). The target variable was the \texttt{price} column, while $X$ contained all other columns used for prediction.
    
    \item \textbf{Encoding Categorical Data:} All categorical features were converted from text labels to numbers using label encoding. Machine learning algorithms, including Random Forest, require numerical input.
    
    \item \textbf{Train-Test Split:} The dataset was divided into training and testing sets. The training set contained 80\% of the data for model learning, and the test set contained 20\% to evaluate model performance on unseen data.
\end{enumerate}

Each preprocessing step was chosen to ensure that the data would be suitable for Random Forest regression, which works best with clean, relevant, and numerical data.

% --------------------------------------------------------------------

\section{Machine Learning Method}

Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. Instead of relying on a single decision tree, it builds many trees and averages their results to get a final prediction.

The algorithm creates multiple random subsets of the training data. Each subset is created by randomly selecting samples with replacement (some samples can appear multiple times, others not at all).

For each data subset, a decision tree is built. However, when splitting nodes in the tree, only a random subset of features is considered at each split point. This makes the trees different from each other.

When we want to predict a value for new data, all trees make their own predictions. The final prediction is the average of all tree predictions.

Mathematically, if we have $N$ decision trees in the forest, and each tree $i$ makes a prediction $\hat{y}_i(x)$ for an input $x$, the final prediction $\hat{y}(x)$ is given by:

\[
\hat{y}(x) = \frac{1}{N} \sum_{i=1}^{N} \hat{y}_i(x)
\]

This averaging helps to reduce overfitting and improves generalization compared to a single decision tree.

Random Forest Regression has several important parameters that control how the model is built:

\begin{description}
    \item[\texttt{n\_estimators}] 
    This parameter defines the number of trees in the forest. A larger number usually improves performance, but it also increases training time. In practice, values between 100 and 300 are often used.

    \item[\texttt{max\_depth}] 
    This parameter sets the maximum depth of each tree. Limiting the depth can prevent overfitting. However, if set too low, it may lead to underfitting.

    \item[\texttt{min\_samples\_split}] 
    This is the minimum number of samples required to split an internal node. Higher values prevent the tree from learning too specific patterns.

    \item[\texttt{min\_samples\_leaf}] 
    This parameter defines the minimum number of samples in a leaf node. It helps to smooth the predictions and reduce noise.

    \item[\texttt{max\_features}] 
    This defines how many features are randomly selected at each split.

    \item[\texttt{random\_state}] 
    This parameter is used to make the results reproducible. Setting it to a fixed number ensures that the same random choices are made each time the model is trained.
\end{description}

Random Forest structure is defined by a collection of independent decision trees. Each tree has a hierarchical structure with internal decision nodes and leaf nodes that store prediction values.

% --------------------------------------------------------------------

\section{Experiments and Results (2--3 pages)}
\begin{itemize}[leftmargin=*]
    \item Splitting into training, validation, and test sets.
    \item Baseline, hyperparameter tuning, cross-validation (if appropriate).
    \item Selected metrics:
    \begin{itemize}
        \item regression: MSE, RMSE, MAE, $R^2$, Adjusted $R^2$,
        \item classification: accuracy, precision, recall, F1, AUC-ROC, confusion matrix.
    \end{itemize}
    \item Plots: learning curves, ROC, predicted vs. actual values.
    \item Interpretation of results â€” what they mean and why.
\end{itemize}

% --------------------------------------------------------------------

\section{Discussion (1 page)}
\begin{itemize}[leftmargin=*]
    \item Evaluation of strengths and weaknesses.
    \item What worked well and what did not, and why.
    \item Possible alternatives and future improvements.
\end{itemize}

% --------------------------------------------------------------------

\section{Conclusion (0.5--1 page)}
\begin{itemize}[leftmargin=*]
    \item Summary of approach and key findings.
    \item Whether the objective was met and why.
    \item Short summary of lessons learned.
\end{itemize}

% --------------------------------------------------------------------

\section*{Appendices}

\section*{Appendix: Dataset Description Plots}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/exploratory-analysis-price-distribution-histogram.png}
    \caption{Price distribution of airline tickets.}
    \label{fig:price_distribution}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/numerical-features-distribution.png}
    \caption{Other numerical attributes distribution.}
    \label{fig:numerical-features-distribution}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/categorical-features-distribution.png}
    \caption{Categorical attributes distribution.}
    \label{fig:categorical-features-distribution}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/correlation-matrix.png}
    \caption{Correlation matrix.}
    \label{fig:correlation-matrix}
\end{figure}

\section*{Appendix: Data Profile}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report01.png}
    \caption{Data Profile Report - part 1.}
    \label{fig:data-profile-report1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report02.png}
    \caption{Data Profile Report - part 2.}
    \label{fig:data-profile-report2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report03.png}
    \caption{Data Profile Report - part 3.}
    \label{fig:data-profile-report3}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report04.png}
    \caption{Data Profile Report - part 4.}
    \label{fig:data-profile-report4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report05.png}
    \caption{Data Profile Report - part 5.}
    \label{fig:data-profile-report5}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report06.png}
    \caption{Data Profile Report - part 6.}
    \label{fig:data-profile-report6}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report07.png}
    \caption{Data Profile Report - part 7.}
    \label{fig:data-profile-report7}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/data-profile-report08.png}
    \caption{Data Profile Report - part 8.}
    \label{fig:data-profile-report8}
\end{figure}

\section*{Declaration of Generative AI Usage}

As part of this semester project, I used generative AI tools (GitHub Copilot) for the following purposes: \
\begin{itemize}
    \item consulting theoretical explanations and verifying correctness,
    \item drafting or reviewing parts of the code,
    \item language proofreading of the text.
\end{itemize}
I fully understand all methods, code, and interpretations used in this work and can independently explain them during the oral exam. I acknowledge that I bear full responsibility for the correctness of all content, calculations, code, and conclusions presented in this project.

\end{document}